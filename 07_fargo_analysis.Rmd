---
title: "SFPA Player Ratings, v.1"
author: "Skip Perry"
date: "March 2019"
output: html_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)

source("02_data_clean.R")
```

```{r, echo=FALSE}
count_games <- function(player_of_interest, results_df) {
  results_df %>% 
    filter(away == player_of_interest | home == player_of_interest) %>% 
    count() %>% 
    transmute(player = player_of_interest, games_in_system = n) %>% 
    pull()
}

games_in_system <- 
  bind_rows(
    results_no_forfeits %>% select(player = home),
    results_no_forfeits %>% select(player = away)
  ) %>% 
  distinct() %>% 
  arrange(player) %>% 
  mutate(games_in_system = 0)

for (i in 1:nrow(games_in_system)) {
  games_in_system$games_in_system[i] <- 
    count_games(games_in_system$player[i], results_no_forfeits)
}
```

```{r, echo=FALSE}
team_names_by_player <- 
  bind_rows(
    results_19_no_forfeits %>% select(player = home, team = home_team),
    results_19_no_forfeits %>% select(player = away, team = away_team)
  ) %>% 
    distinct() %>% 
    arrange(player, team)

fargo_df <- 
  readRDS("other_data/fargo_324.Rdata") %>% 
  transmute(player, rating, match_date)

fargo_df_latest <-
  fargo_df %>% 
  filter(match_date == fargo_df %>% select(match_date) %>% pull() %>% max()) %>% 
  arrange(desc(rating)) %>% 
  mutate(rank = row_number()) %>% 
  left_join(team_names_by_player, by = "player") %>% 
  left_join(games_in_system, by = "player") %>% 
  transmute(
    rank, 
    player, 
    current_team = replace_na(team, "--"), 
    rating = round(rating),
    games_in_system
  )
```

These ratings are based on the Bradley-Terry model, which since the 1950s has been one of the standard methods of rating individuals or teams that are repeatedly paired against one another. Considering two players $i$ and $j$ with ratings $\pi_i$ and $\pi_j$, this framework assumes player $i$ has the following probability of defeating player $j$:

$$\text{P[Player } i \text{ defeats player } j] = p_{i > j} = \frac{\pi_i}{\pi_i + \pi_j}$$

While there is no analytical solution for the maximum likelihood estimate (MLE) of the collection of player ratings $\pi_{i \in [1,n]}$ for players 1 through $n$, iterative methods can be used to find a result:

$$\pi_i^{new} = \frac{\text{# of total wins by player } i}{\sum_{\text{All games played by } i} \frac{\text{# of games between } i, j}{\pi_i^{current} + \pi_j^{current}}}$$

Or, using more traditional mathematical notation, where $\pi_i$ is player $i$'s rating, $w_i$ is the number of times player $i$ won a game, $n_{ij}$ is the number of games played between players $i$ and $j$, and $(t)$ indicates the result at the $t^{th}$ iteration:

$$\pi_i^{(t)} = \frac{w_i}{\sum_{j \neq i} \frac{n_{ij}}{\pi_i^{(t-1)} + \pi_j^{(t-1)}}}$$

This adaptation of the expectation-maximization (EM) algorithm loops through each player, updating their rating using the latest estimates of all their opponents' ratings. (A little experimentation with Google Calculator or Excel will demonstrate how this formula gives players more credit for beating higher-ranked opponents.) Eventually, each update will have such a small effect on the vector of player ratings $\pi$ that we can stop the process. 

Maximum likelihood estimation is the single most widely used method of parameter estimation, but it has problems. While convergence is guaranteed under certain conditions, one deals with having a sufficient number of connections in the data, such as you might find in a series of round-robin games played between players ([see Assumption 3 here](http://personal.psu.edu/drh20/papers/bt.pdf)). Additionally, the MLE converges toward positive (and negative) infinity for unbeaten (and winless) players. The SFPA data set faces both of these issues - sparse data with many players playing only a handful of games against a limited subset of opponents, and some examples of players with no wins at all.

Thankfully, a Bayesian approach can be used to find the maximum a priori (MAP) estimate of **$\pi$** without resorting to omitting offending data or including only results from players with a minimum number of games. We get what we need by setting a $G(a, b)$ prior on $p(\pi)$, which is conjugate to the complete data likelihood function and results in the following update formula for $\pi$:

$$\pi_i^{(t)} = \frac{a - 1 + w_i}{b + \sum_{j \neq i} \frac{n_{ij}}{\pi_i^{(t-1)} + \pi_j^{(t-1)}}}$$

Like the MLE, MAP estimation provides a point estimate, though in this case it is the mode of a posterior distribution rather than the maximization of a likelihood function. (Note that for $a = 1$ and $b = 0$, the MAP and maximum likelihood estimates are equivalent; see [here](http://www.stats.ox.ac.uk/~doucet/caron_doucet_bayesianbradleyterry.pdf) for more information.) Setting $b = a / 500$ leads to a mean player rating of around 500 while satisfying identifiability constraints. Hyperparameter tuning for $a$ resulted in a choice of 2.75 - a weakly informative prior.

Other considerations included:

* Home-table advantage: In the past three seasons, about 52% of games were won by the home team, a small but significant difference. There also exists a simple way to incorporate the home-table advantage into the ratings, setting $p_{i>j} = \frac{\theta \pi_i}{\theta \pi_i + \pi_j}$ where $\theta > 1$ is the home-table advantage (or less than 1 if it's a disavantage). Unfortunately, this is a noisy input: due to scheduling conflicts and bar remodeling, teams often play "home" games at other bars; most week 1 games are home games for both teams; and playoff games represent an uneven playing field since higher-seeded teams play at home while lower-seeded teams play on the road, among other factors. After quite a bit of experimentation in this area failed to improve model performance, I omitted the home-table advantage from the rating system.

* Time decay: There are a few different ways of handling ratings that change over time. One extreme is to count every match someone has ever played; another would be to set a hard cutoff and only count the most recent, say, 50 or 75 games played. The middle ground used here is a sliding scale that results in older matches contribute less to a player's rating than newer ones. Matches are given a half-life of 1500 days from the date of a player's latest game. (As an example, matches played in January 2018 contribute about 80% of the value of a match played in March 2019.) Since parameter tuning showed little overall impact on the ratings as a whole, this number was chosen arbitrarily. The end result is a rating system that is responsive to change but results in fewer wild swings, especially for players with a lot of data.

* Robustness: In order to prevent new players from affecting the ratings of established players, games against new players are down-weighted until they reach a certain number of games. As with the time decay, experimentation showed that different values of this parameter had little impact on the ultimate results. The number 10 was chosen based on an exploratory examination of when many players' ratings begin to converge to a relatively steady-state level.  

Without further ado, the ratings as of March 19, 2019:

```{r, echo=FALSE}
fargo_df_latest %>% 
  arrange(desc(rating)) %>% 
  knitr::kable()
```

