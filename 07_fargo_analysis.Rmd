---
title: "SFPA Player Ratings, v.1"
author: "Skip Perry"
date: "March 2019"
output: html_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)

source("02_data_clean.R")
```

```{r, echo=FALSE}
count_games <- function(player_of_interest, results_df) {
  results_df %>% 
    filter(away == player_of_interest | home == player_of_interest) %>% 
    count() %>% 
    transmute(player = player_of_interest, games_in_system = n) %>% 
    pull()
}

games_in_system <- 
  bind_rows(
    results_no_forfeits %>% select(player = home),
    results_no_forfeits %>% select(player = away)
  ) %>% 
  distinct() %>% 
  arrange(player) %>% 
  mutate(games_in_system = 0)

for (i in 1:nrow(games_in_system)) {
  games_in_system$games_in_system[i] <- 
    count_games(games_in_system$player[i], results_no_forfeits)
}
```

```{r, echo=FALSE}
team_names_by_player <- 
  bind_rows(
    results_19_no_forfeits %>% select(player = home, team = home_team),
    results_19_no_forfeits %>% select(player = away, team = away_team)
  ) %>% 
    distinct() %>% 
    arrange(player, team)

fargo_df <- 
  readRDS("other_data/fargo_324.Rdata") %>% 
  transmute(player, rating, match_date)

fargo_df_latest <-
  fargo_df %>% 
  filter(match_date == fargo_df %>% select(match_date) %>% pull() %>% max()) %>% 
  arrange(desc(rating)) %>% 
  mutate(rank = row_number()) %>% 
  left_join(team_names_by_player, by = "player") %>% 
  left_join(games_in_system, by = "player") %>% 
  transmute(
    rank, 
    player, 
    current_team = replace_na(team, "--"), 
    rating = round(rating),
    games_in_system
  )
```

### The Math Behind the Model 

These ratings are based on the Bradley-Terry model, which since the 1950s has been one of the standard methods of rating individuals or teams that are repeatedly paired against one another. Considering two players $i$ and $j$ with ratings $\pi_i$ and $\pi_j$, this framework assumes player $i$ has the following probability of defeating player $j$:

$$\text{P[Player } i \text{ defeats player } j] = p_{i > j} = \frac{\pi_i}{\pi_i + \pi_j}$$

While there is no analytical solution for the maximum likelihood estimate (MLE) of the collection of player ratings $\pi_{i \in [1,n]}$ for players 1 through $n$, iterative methods can be used to find a result:

$$\pi_i^{new} = \frac{\text{# of total wins by player } i}{\sum_{\text{All games played by } i} \frac{\text{# of games between } i, j}{\pi_i^{current} + \pi_j^{current}}}$$

Or, using more traditional mathematical notation, where $\pi_i$ is player $i$'s rating, $w_i$ is the number of times player $i$ won a game, $n_{ij}$ is the number of games played between players $i$ and $j$, and $(t)$ indicates the result at the $t^{th}$ iteration:

$$\pi_i^{(t)} = \frac{w_i}{\sum_{j \neq i} \frac{n_{ij}}{\pi_i^{(t-1)} + \pi_j^{(t-1)}}}$$

This adaptation of the expectation-maximization (EM) algorithm loops through each player, updating their rating using the latest estimates of all their opponents' ratings. (A little experimentation with Google Calculator or Excel will demonstrate how this formula gives players more credit for beating higher-ranked opponents.) Eventually, each update will have such a small effect on the vector of player ratings $\pi$ that we can stop the process. 

Maximum likelihood estimation is the single most widely used method of parameter estimation, but it is far from perfect. While convergence is guaranteed under certain conditions, one of those conditions deals with having a sufficient number of connections in the data, such as you might find in a series of round-robin games played between all teams in a league ([see Assumption 3 here](http://personal.psu.edu/drh20/papers/bt.pdf)). Additionally, the MLE can converge toward the boundary of a parameter space when dealing with extreme data, which in this case would mean a rating of 0 for a winless player and a rating of infinity for an undefeated one. This presents a clear problem when plugging ratings into the formula $\frac{\pi_i}{\pi_i + \pi_j}$.  The SFPA data set faces both of these issues - sparse data with many league members playing only a handful of games against a limited subset of opponents, and some examples of players with no wins at all.

Bayesian methods allow us to avoid these problems.  Instead of finding an MLE after taking radical steps like omitting winless players or including only results from players with a minimum number of games, we calculate the maximum a priori (MAP) estimate of **$\pi$** by setting a $\Gamma(a, b)$ prior on $\pi$, where $\Gamma$ indicates the gamma probability distribution with parameters $a$ and $b$. This prior is conjugate to the complete data likelihood function $P(X \vert \pi)$ and results in the following update formula:

$$\pi_i^{(t)} = \frac{a - 1 + w_i}{b + \sum_{j \neq i} \frac{n_{ij}}{\pi_i^{(t-1)} + \pi_j^{(t-1)}}}$$

Like the MLE, MAP estimation provides a point estimate, though in this case it is the mode of a posterior distribution rather than the maximization of a likelihood function. (Note that for $a = 1$ and $b = 0$, the MAP and maximum likelihood estimates are equivalent; see [here](http://www.stats.ox.ac.uk/~doucet/caron_doucet_bayesianbradleyterry.pdf) for more information.) 

Under this setup, as in many Bayesian appication, the choice of prior $a$ has a major impact on the model. When $a = 1$, we simply have the MLE; as the value of $a$ increases, the impact of actual game results decreases as the numerator and denominator in the update formula become dominated by the constant $a-1$ and $b$ terms. Hyperparameter tuning for $a$ resulted in an optimal choice of 2.75 - a weakly informative prior. 

The scale of these ratings is arbitrary; any set of ratings can be multiplied by some positive number and result in the same probability $p_{i>j}$ for all $i$ and $j$. As a result, in the MLE context, we would need to peg a player rating $\pi_i$ to a particular value, or set up an additional constraint such as $1 / n * \sum_{i=1}^{n} \pi_i = 1000$, in order to arrive at a unique solution. In the MAP formulation, we can satisfy this identifiability requirement by setting $b = (a - 1) / 500$ and ensure a mean player rating of roughly 500 in the process.

Other considerations included:

* Home-table advantage: In the past three seasons, about 52% of games were won by the home team, a small but significant edge to the home player. There also exists a simple way to incorporate the home-table advantage into the ratings, setting $p_{i>j} = \frac{\theta \pi_i}{\theta \pi_i + \pi_j}$ where $\theta > 1$ is the home-table advantage (or less than 1 if it's a disavantage). Unfortunately, this is a noisy input: due to scheduling conflicts and bar remodeling, teams often play "home" games at other bars; most week 1 games are home games for both teams; and playoff games represent an uneven playing field since higher-seeded teams play at home while lower-seeded teams play on the road, among other factors. After quite a bit of experimentation in this area failed to improve model performance, I omitted the home-table advantage from the rating system.

* Time decay: In pool, a player's skill level can sometimes change over time. One extreme would be to assume that these changes are the result of random noise and place equal weight on every game in a player's match history; another would be to use a subset of a player's most recent games, on the assumption that those games most accurately reflect their skill. The middle ground solution used here is a sliding scale where older matches contribute less to a player's rating than newer ones. Matches are given a weighting factor between 0 and 1 based on a half-life of 1500 days from the date of a player's latest game. (As an example, for the latest March 2019 ratings, matches played in January 2018 contribute about 80% of the value of a match played in the current month.) Since parameter tuning on the number of days to half-life showed little impact on model results, this number was chosen arbitrarily. The end result is a rating system that is responsive to change but results in fewer wild temporary swings, especially for players with more established ratings. 

* Robustness: In order to prevent new players, whose ratings are based on a small amount of data, from too greatly affecting the ratings of established players, games against newly-joined league members are down-weighted until those players have competed in a certain number of games. As with the time decay, experimentation with different reasonable values of this parameter showed a negligible impact on model performance. The number 10 was chosen based on an exploratory analysis of when players' ratings begin to converge to a relative steady state.

### Questions and Answers 

What do the ratings mean? 

* The only meaningful number in the ratings is 500, which has been set as the rough global league average. Otherwise, the ratings are only useful to calculate individual match probabilities: the probability of player A beating player B in a single game is equal to $A / (A + B)$. 

How and why does my rating go up or down?

* Your rating will go up when you perform better than the current ratings predict, and it will go down if you perform worse than the current ratings predict. Your rating will also change based on changes in your opponents' ratings. Let's assume you have a 6-3 record against some previous opponent. If that opponent has a few great nights and his or her rating increases, you will earn more points for having done well against that person. (The converse is true if your previous opponent's rating decreases.)

How much do ratings change over time?

* Some players perform at a consistent level and show only minor changes over time. Other people have seen large shifts in their ratings, both positive and negative. Substantial changes in players' ratings are almost always due to changes in their own performance, rather than changes in the performance of their previous opponents. 

Do new league players have starter ratings?

* Not exactly. Players start out with a rating of 500, but from the very beginning their match results start to affect their ratings. Instead of holding a new player to an arbitrary starter rating for some arbitrary period of time, we allow the results to follow the data even if that data is noisy at first.

How many games are needed to indicate that a rating is trustworthy?

* This is not a simple question. We are in the process of generating confidence intervals (or, more accurately, Bayesian credible intervals) for ratings to give an idea of how sure we may be. Initial exploration suggests 10 games is the time period by which many players tend see their ratings plateau. 

### Ratings as of March 19, 2019

```{r, echo=FALSE}
fargo_df_latest %>% 
  arrange(desc(rating)) %>% 
  knitr::kable()
```

