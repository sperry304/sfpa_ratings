---
title: "SFPA Player Ratings, v.1"
author: "Skip Perry"
date: "March 2019"
output: html_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)

source("02_data_clean.R")
```

```{r, echo=FALSE}
count_games <- function(player_of_interest, results_df) {
  results_df %>% 
    filter(away == player_of_interest | home == player_of_interest) %>% 
    count() %>% 
    transmute(player = player_of_interest, games_in_system = n) %>% 
    pull()
}

games_in_system <- 
  bind_rows(
    results_no_forfeits %>% select(player = home),
    results_no_forfeits %>% select(player = away)
  ) %>% 
  distinct() %>% 
  arrange(player) %>% 
  mutate(games_in_system = 0)

for (i in 1:nrow(games_in_system)) {
  games_in_system$games_in_system[i] <- 
    count_games(games_in_system$player[i], results_no_forfeits)
}
```

```{r, echo=FALSE}
team_names_by_player <- 
  bind_rows(
    results_19_no_forfeits %>% select(player = home, team = home_team),
    results_19_no_forfeits %>% select(player = away, team = away_team)
  ) %>% 
    distinct() %>% 
    arrange(player, team)

fargo_df <- 
  readRDS("other_data/fargo_324.Rdata") %>% 
  transmute(player, rating, match_date)

fargo_df_latest <-
  fargo_df %>% 
  filter(match_date == fargo_df %>% select(match_date) %>% pull() %>% max()) %>% 
  arrange(desc(rating)) %>% 
  mutate(rank = row_number()) %>% 
  left_join(team_names_by_player, by = "player") %>% 
  left_join(games_in_system, by = "player") %>% 
  transmute(
    rank, 
    player, 
    current_team = replace_na(team, "--"), 
    rating = round(rating),
    games_in_system
  )
```

These ratings are based on the Bradley-Terry model, which since the 1950s has been one of the standard methods of rating individuals or teams that are repeatedly paired against one another. Considering two players $i$ and $j$ with ratings $\pi_i$ and $\pi_j$, this framework assumes player $i$ has the following probability of defeating player $j$:

$$\text{P[Player } i \text{ defeats player } j] = p_{i > j} = \frac{\pi_i}{\pi_i + \pi_j}$$

While there is no analytical solution for the maximum likelihood estimate (MLE) of the collection of player ratings $\pi_{i \in [1,n]}$ for players 1 through $n$, iterative methods can be used to find a result:

$$\pi_i^{new} = \frac{\text{# of total wins by player } i}{\sum_{\text{All games played by } i} \frac{\text{# of games between } i, j}{\pi_i^{current} + \pi_j^{current}}}$$

Or, using more traditional mathematical notation, where $\pi_i$ is player $i$'s rating, $w_i$ is the number of times player $i$ won a game, $n_{ij}$ is the number of games played between players $i$ and $j$, and $(t)$ indicates the result at the $t^{th}$ iteration:

$$\pi_i^{(t)} = \frac{w_i}{\sum_{j \neq i} \frac{n_{ij}}{\pi_i^{(t-1)} + \pi_j^{(t-1)}}}$$

This adaptation of the expectation-maximization (EM) algorithm loops through each player, updating their rating using the latest estimates of all their opponents' ratings. (A little experimentation with Google Calculator or Excel will demonstrate how this formula gives players more credit for beating higher-ranked opponents.) Eventually, each update will have such a small effect on the vector of player ratings $\pi$ that we can stop the process. 

Maximum likelihood estimation is the single most widely used method of parameter estimation, but it has problems. While convergence is guaranteed under certain conditions, one deals with having a sufficient number of connections in the data, such as you might find in a series of round-robin games played between players ([see Assumption 3 here](http://personal.psu.edu/drh20/papers/bt.pdf)). Additionally, the MLE converges toward positive (and negative) infinity for unbeaten (and winless) players. The SFPA data set faces both of these issues - sparse data with many players playing only a handful of games against a limited subset of opponents, and some examples of players with no wins at all.

Thankfully, a Bayesian approach can be used to find the maximum a priori (MAP) estimate of **$\pi$** without resorting to omitting offending data or including only results from players with a minimum number of games. We get what we need by setting a $G(a, b)$ prior on $p(\pi)$, which is conjugate to the complete data likelihood function and results in the following update formula for $\pi$:

$$\pi_i^{(t)} = \frac{a - 1 + w_i}{b + \sum_{j \neq i} \frac{n_{ij}}{\pi_i^{(t-1)} + \pi_j^{(t-1)}}}$$

Like the MLE, MAP estimation provides a point estimate, though in this case it is the mode of a posterior distribution rather than the maximization of a likelihood function. (Note that for $a = 1$ and $b = 0$, the MAP and maximum likelihood estimates are equivalent; see [here](http://www.stats.ox.ac.uk/~doucet/caron_doucet_bayesianbradleyterry.pdf) for more information.) 

Under this setup, the choice of $a$ has a major impact on the model. When $a = 1$, we simply have the MLE; with a large value of $a$, each additional game played has a small impact on the player's rating as the numerator and denominator are dominated by $a - 1$ and $b$. Hyperparameter tuning for $a$ resulted in an optimal choice of 2.75 - a weakly informative prior. 

The scale of these ratings is arbitrary; any set of ratings can be multiplied by some positive number and result in the same probability $p_{i>j}$ for all $i$ and $j$. As a result, in the MLE context, we would need to peg a player rating $\pi_i$ to a particular value, or set up an additional constraint such as $\sum_{i=1}^{n} p_i = 1$, in order to arrive at a unique solution. In the MAP formulation, we can satisfy this identifiability requirement by setting $b = (a - 1) / 500$ and arrive at a mean player rating of roughly 500 in the process.

Other considerations included:

* Home-table advantage: In the past three seasons, about 52% of games were won by the home team, a small but significant edge to the home player. There also exists a simple way to incorporate the home-table advantage into the ratings, setting $p_{i>j} = \frac{\theta \pi_i}{\theta \pi_i + \pi_j}$ where $\theta > 1$ is the home-table advantage (or less than 1 if it's a disavantage). Unfortunately, this is a noisy input: due to scheduling conflicts and bar remodeling, teams often play "home" games at other bars; most week 1 games are home games for both teams; and playoff games represent an uneven playing field since higher-seeded teams play at home while lower-seeded teams play on the road, among other factors. After quite a bit of experimentation in this area failed to improve model performance, I omitted the home-table advantage from the rating system.

* Time decay: In pool, a player's skill level can sometimes change over time. One extreme would be to assume a that these changes are the result of random noise and place equal weight on every match someone has ever played; another would be to use a subset of a player's most recent games, on the assumption that they most accurately reflect their skill. The middle ground solution used here is a sliding scale where older matches contribute less to a player's rating than newer ones. Matches are given a weighting factor between 0 and 1 based on a half-life of 1500 days from the date of a player's latest game. (As an example, for the latest March 2019 ratings, matches played in January 2018 contribute about 80% of the value of a match played in the current month.) Since parameter tuning on the number of days to half-life showed little impact on model results, this number was chosen arbitrarily. The end result is a rating system that is responsive to change but results in fewer wild temporary swings, especially for players with more established ratings. 

* Robustness: In order to prevent new players, whose ratings are based on a small amount of data, from too greatly affecting the ratings of established players, games against newly-joined league members are down-weighted until those players have competed in a certain number of games. As with the time decay, experimentation with different reasonable values of this parameter showed a negligible impact on model performance. The number 10 was chosen based on an exploratory analysis of when players' ratings begin to converge to a relative steady state.

Finally, a few words on why this model is preferable to the often-used ELO ratings:

* ELO is highly momentum-based.

* The ELO update formula essentially works as if players ante poker chips into a pot, with differential win and loss numbers based on each player's rating. Thus, every time you lose, your  

Without further ado, the ratings as of March 19, 2019:

```{r, echo=FALSE}
fargo_df_latest %>% 
  arrange(desc(rating)) %>% 
  knitr::kable()
```

